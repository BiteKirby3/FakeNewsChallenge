{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30413cc7d415478cadc636ba589d2bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8598aa0b3fce41a78fb4c719aa6b2a75",
              "IPY_MODEL_c50323aa7fc94278a2f42dd347b3a5dd",
              "IPY_MODEL_2eac8612768042a188f0a408ee02b06c"
            ],
            "layout": "IPY_MODEL_79f7bd98a38e41e5acf12748b76c62de"
          }
        },
        "8598aa0b3fce41a78fb4c719aa6b2a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_813c76f75a004719acbf75d41588f867",
            "placeholder": "​",
            "style": "IPY_MODEL_fb13529f200148c89ab042699ae429bd",
            "value": "Downloading builder script: 100%"
          }
        },
        "c50323aa7fc94278a2f42dd347b3a5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b45bb91236f644bab0e29ea0601c9c60",
            "max": 6771,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12162002c533420ca84cd6e0dfacbe70",
            "value": 6771
          }
        },
        "2eac8612768042a188f0a408ee02b06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bce219a5e2564a1bbfcf8f0a4f10bca9",
            "placeholder": "​",
            "style": "IPY_MODEL_505704d2798a4801a0bb3223b220d806",
            "value": " 6.77k/6.77k [00:00&lt;00:00, 329kB/s]"
          }
        },
        "79f7bd98a38e41e5acf12748b76c62de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813c76f75a004719acbf75d41588f867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb13529f200148c89ab042699ae429bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b45bb91236f644bab0e29ea0601c9c60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12162002c533420ca84cd6e0dfacbe70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bce219a5e2564a1bbfcf8f0a4f10bca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505704d2798a4801a0bb3223b220d806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wwJ8mlYgh31H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d385f1-f517-42ec-d527-981fcb093907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'FakeNewsChallenge'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 97 (delta 40), reused 71 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (97/97), 5.06 MiB | 5.74 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "%cd /content\n",
        "!git clone --recursive https://github.com/BiteKirby3/FakeNewsChallenge\n",
        "root_dir = \"/content/FakeNewsChallenge/fnc-1-baseline\"\n",
        "os.chdir(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import scipy \n",
        "import nltk\n",
        "from datetime import date\n",
        "import csv"
      ],
      "metadata": {
        "id": "cJzrHJjO_XIA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Notebook, we use [OpenAI](https://platform.openai.com/docs/api-reference) API to solve the Fake News Challenge Stage 1 ([FNC-I](http://www.fakenewschallenge.org/)) - stance detection task. To call the API of OpenAI, you need to have your secret API key."
      ],
      "metadata": {
        "id": "Xn4mcbg20bEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = '<YOUR_API_KEY>'"
      ],
      "metadata": {
        "id": "KOsY-9p_zsJf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "CQp_Uv310dFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading"
      ],
      "metadata": {
        "id": "siuH72ib03DN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the stances and article bodies into two separate containers."
      ],
      "metadata": {
        "id": "6PPvh_7C24Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.dataset import DataSet"
      ],
      "metadata": {
        "id": "yfC3s2b0lPWE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = DataSet(\"train\")\n",
        "dataset_test = DataSet(\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iXnIBZMjZEP",
        "outputId": "fc051ae1-faf3-4b11-a901-ae73c8aa743a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading dataset\n",
            "Total stances: 49972\n",
            "Total bodies: 1683\n",
            "Reading dataset\n",
            "Total stances: 25413\n",
            "Total bodies: 904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access it through the *.stances* and *.articles* variables. Moreover, *.articles* is a dictionary of articles, indexed by the body id."
      ],
      "metadata": {
        "id": "bU72OTgn3Bck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.stances[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YazbeUOb2O2e",
        "outputId": "0d8e2918-3677-4a7e-e772-bb229de10641"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Headline': 'Ferguson riots: Pregnant woman loses eye after cops fire BEAN BAG round through car window',\n",
              " 'Body ID': 2008,\n",
              " 'Stance': 'unrelated'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_test.articles[dataset_test.stances[0]['Body ID']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYSSARME2daw",
        "outputId": "bf88a3d0-3c6e-4c20-a9cd-b9d819c9fb2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A RESPECTED senior French police officer investigating the Charlie Hebdo magazine massacre took his own life mere hours after the horrific attacks stunned the world.\n",
            "\n",
            "Commissioner Helric Fredou, 45, turned a gun on himself in his police office in Limoges last Wednesday night, reported France 3.\n",
            "\n",
            "A colleague found his body at 1am on Thursday, the day after three gunmen fired at the satirical magazine's office and left 12 people dead.\n",
            "\n",
            "Speaking to our sister publication Mirror Online, the Union of Commissioners of the National Police confirmed Mr Fredou had taken his own life.\n",
            "\n",
            "\n",
            "In a statement released after his death, a union spokesman said: \"It is with great sadness that we were informed this morning of the death of our colleague Helric Fredou, assigned as Deputy Director of the Regional Service Judicial Police in Limoges.\n",
            "\n",
            "\"On this particular day of national mourning, police commissioners are hit hard by the tragic death of one of their own.\n",
            "\n",
            "\"The Union of Commissioners of the National Police would like to present its most sincere condolences to the relatives of Helric.\n",
            "\n",
            "\"In these difficult times, we have a special thought for all his colleagues.\"\n",
            "\n",
            "Mr Fredou - who was single and had no children - began his career as a police officer in 1997, working in Versailles.\n",
            "\n",
            "He eventually returned to his home town of Limoges and in 2012 became deputy director of the regional police service.\n",
            "\n",
            "\n",
            "French media reports suggest he was depressed and was suffering from burnout.\n",
            "\n",
            "Brothers Said and Cherif Kouachi launched last Wednesday's devastating attack at the office's of the French satirical magazine.\n",
            "\n",
            "The attack left 12 people dead. On Friday, the pair were shot dead by police after taking a man hostage at a family-run printing press in Dammartin-en-Goele .\n",
            "\n",
            "In a separate incident on Friday, Amedy Coulibaly, 32, took about 15 people hostage in a Paris supermarket.\n",
            "\n",
            "Four hostages were killed in the incident along with Coulibaly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Construct a 1000-instance test dataset"
      ],
      "metadata": {
        "id": "aatXjsmNIOWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct a 1000-instance test dataset, where we have 731 unrelated, 178 discuss, 17 disagree and 74 agree stance examples."
      ],
      "metadata": {
        "id": "V1-hSkR8Ic_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_UNRELATED = 731\n",
        "NB_DISCUSS = 178\n",
        "NB_DISAGREE = 17\n",
        "NB_AGREE = 74\n",
        "cpt_unrelated,cpt_discuss,cpt_disagree,cpt_agree = 0,0,0,0\n",
        "testset_index = []"
      ],
      "metadata": {
        "id": "5dXMwljGNJNQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(dataset_test.stances)):\n",
        "  if(dataset_test.stances[i]['Stance']=='unrelated' and cpt_unrelated<NB_UNRELATED):\n",
        "    testset_index.append(i)\n",
        "  elif(dataset_test.stances[i]['Stance']=='discuss' and cpt_discuss<NB_DISCUSS):\n",
        "    testset_index.append(i)\n",
        "  elif(dataset_test.stances[i]['Stance']=='agree' and cpt_agree<NB_AGREE):\n",
        "    testset_index.append(i)\n",
        "  elif(dataset_test.stances[i]['Stance']=='disagree' and cpt_disagree<NB_DISAGREE):\n",
        "    testset_index.append(i)\n",
        "  if len(testset_index) == 1000:\n",
        "    break"
      ],
      "metadata": {
        "id": "7rAXYUrBOtwB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-train, Prompt and Predict Paradigm"
      ],
      "metadata": {
        "id": "qlp7S5k6Eaoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is rather intuitive and simple to apply, in our case, news stance classification can be viewed as asking chatGPT a question, for example,\n",
        "\n",
        "**What's the stance of the news body:**\n",
        "$<Body>$\n",
        "\n",
        "**to the news headline:**\n",
        "$<Headline>$? \n",
        "\n",
        "**Choose a stance from \"unrelated, discuss, agree, disagree\". The stance is**\n",
        "\n",
        "Then wait for the completion of the sentence answered by chatGPT."
      ],
      "metadata": {
        "id": "_BDsCa2pFl2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be implemented through OpenAI's officiel API. \n",
        "We define a prompt template that includes the news headline as well as the news body and asks for the stance of the news body towards it. Then we filter the generated stances to select the relevant stance.(See OpenAI's [Chat Completion API](https://platform.openai.com/docs/api-reference/chat) for the detailed usages.)"
      ],
      "metadata": {
        "id": "K458eS3vVabP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the quality of classification depends on the quality of the prompt, thus it may require some fine-tuning to obtain accurate results."
      ],
      "metadata": {
        "id": "LvuVUIbxtevw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose `gpt-3.5-turbo` model, which is the most capable [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5) family model and is optimized for chat at 1/10th the cost of `text-davinci-003`. Since OpenAI limits the access rate to the API for normal users(3 requests/minute), we only test on a smaller dataset which contains only 1000 instances but keeps approximately the same proportion for each class as the original test dataset."
      ],
      "metadata": {
        "id": "hkHx09m3Is5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOw2sjysEGId",
        "outputId": "c609e1dc-05b5-4b57-b044-15dfdb87db3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "xojKYqsbRbb1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = API_KEY"
      ],
      "metadata": {
        "id": "TxTdi2ftF5Sl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zero-Shot prompt"
      ],
      "metadata": {
        "id": "a5SH3zntIb8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-Shot prompt means classifying directly previously unseen text into categories that the model has never been explicitly trained to identify. Usually, the zero-shot prompt-based classifier needs proper prompt engineering to achieve high accuracy."
      ],
      "metadata": {
        "id": "fMT3ht5Xk5G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_stance_zero_shot(news_body, news_headline):\n",
        "    # Define prompt template\n",
        "    headline_template = \"Given the following news headline: '{}'.\"\n",
        "    body_template = \"Given the following news body: '{}'.\"\n",
        "    question_template = \"What is the stance of this news body towards this news headline? Please choose one of the following stances: unrelated, discuss, agree, disagree. The stance is \"\n",
        "\n",
        "    # Generate prompt\n",
        "    news_body = news_body.replace(\"\\n\\n\",\"\\n\")\n",
        "    news_body = news_body.replace(\"\\n\\n\",\"\\n\")\n",
        "    headline_template = headline_template.format(news_headline)\n",
        "    body_template = body_template.format(news_body)\n",
        "\n",
        "    # Generate possible stances using ChatGPT\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": headline_template},\n",
        "                  {\"role\": \"user\", \"content\": body_template},\n",
        "                  {\"role\": \"user\", \"content\": question_template}\n",
        "                  ],\n",
        "        temperature = 0.5\n",
        "    )\n",
        "\n",
        "    # Filter stances to select the most relevant one\n",
        "    stance = response.choices[0].message.content\n",
        "    if stance[-1]==\".\" :\n",
        "      stance = stance.rstrip(stance[-1])\n",
        "    #if stance in [\"unrelated\", \"discuss\", \"agree\", \"disagree\"]:\n",
        "    #    return stance\n",
        "    #else:\n",
        "    #    return None\n",
        "    return stance"
      ],
      "metadata": {
        "id": "S8OZDiyWwSoi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test \n",
        "print(dataset_test.stances[248])\n",
        "print(\"The stance given by GPT3.5 is: \"+classify_stance_zero_shot(dataset_test.articles[dataset_test.stances[248]['Body ID']], dataset_test.stances[248]['Headline']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada50c5d-ae91-48cb-c179-bfd9a847eb4d",
        "id": "HXzcXeyBdapw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Headline': '‘Crabzilla’ spotted off the coast of Britain', 'Body ID': 893, 'Stance': 'disagree'}\n",
            "The stance given by GPT3.5 is: disagree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classify the test dataset, we write the GPT responses to a csv file.\n",
        "import time\n",
        "filename = \"/content/FakeNewsChallenge/result/\"+\"zero_shot_prompt_prediction\"+str(date.today())+\".csv\"\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "    fieldnames = [\"STANCE_INDEX\",\"ACTUAL_STANCE\",\"PREDICT_STANCE\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in testset_index:\n",
        "      actual_stance = dataset_test.stances[i]['Stance']\n",
        "      headline = dataset_test.stances[i]['Headline']\n",
        "      body = dataset_test.articles[dataset_test.stances[i]['Body ID']]\n",
        "      predict_stance = classify_stance_zero_shot(dataset_test.articles[dataset_test.stances[i]['Body ID']], dataset_test.stances[i]['Headline'])\n",
        "      writer.writerow({'STANCE_INDEX': str(i), 'ACTUAL_STANCE': str(actual_stance), 'PREDICT_STANCE': str(predict_stance)})\n",
        "      csvfile.flush()\n",
        "      time.sleep(21) #wait for 21s to not interrupt API access"
      ],
      "metadata": {
        "id": "i22Roz5AgiB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Few-Shot prompt"
      ],
      "metadata": {
        "id": "K9pU9z9FIojA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot prompting is a technique where the model is given a small number of example(typically between two and five), in order to quickly adapt to new context. Although it requires less data, usually this technique can allow for the generation of more versatile and adaptive text."
      ],
      "metadata": {
        "id": "SSVHDMrBowFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, in addition to the test stance example, we also feed the input a `discuss`, a `unrelated`, an `agree` and a `disagree` stance example from the training set so that the model can learn to adapt to this context."
      ],
      "metadata": {
        "id": "rU6Rn9hBpjq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_stance_few_shot(news_body, news_headline):\n",
        "    # Define prompt template\n",
        "    #One-shot for each stance\n",
        "    #unrelated example\n",
        "    unrelated_example = \"News Headline: {}, News body: {}, Stance: unrelated\".format(dataset_train.stances[0]['Headline'], dataset_train.articles[dataset_train.stances[0]['Body ID']].replace(\"\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\"))\n",
        "    #discuss example\n",
        "    discuss_example = \"News Headline: {}, News body: {}, Stance: discuss\".format(dataset_train.stances[10]['Headline'], dataset_train.articles[dataset_train.stances[10]['Body ID']].replace(\"\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\"))\n",
        "    #disagree example\n",
        "    disagree_example = \"News Headline: {}, News body: {}, Stance: disagree\".format(dataset_train.stances[4]['Headline'], dataset_train.articles[dataset_train.stances[4]['Body ID']].replace(\"\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\"))\n",
        "    #agree example\n",
        "    agree_example = \"News Headline: {}, News body: {}, Stance: agree\".format(dataset_train.stances[1]['Headline'], dataset_train.articles[dataset_train.stances[1]['Body ID']].replace(\"\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\"))\n",
        "\n",
        "    #test example\n",
        "    headline_template = \"Given the following news headline: '{}'.\"\n",
        "    body_template = \"Given the following news body: '{}'.\"\n",
        "    question_template = \"What is the stance of this news body towards this news headline? Please choose one of the following stances: unrelated, discuss, agree, disagree. The stance is \"\n",
        "\n",
        "    # Generate prompt\n",
        "    news_body = news_body.replace(\"\\n\\n\",\"\\n\")\n",
        "    news_body = news_body.replace(\"\\n\\n\",\"\\n\")\n",
        "    headline_template = headline_template.format(news_headline)\n",
        "    body_template = body_template.format(news_body)\n",
        "\n",
        "    # Generate possible stances using ChatGPT\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": unrelated_example},\n",
        "                  {\"role\": \"user\", \"content\": discuss_example},\n",
        "                  {\"role\": \"user\", \"content\": disagree_example},\n",
        "                  {\"role\": \"user\", \"content\": agree_example},\n",
        "                  {\"role\": \"user\", \"content\": headline_template},\n",
        "                  {\"role\": \"user\", \"content\": body_template},\n",
        "                  {\"role\": \"user\", \"content\": question_template}\n",
        "                  ],\n",
        "        temperature = 0.5\n",
        "    )\n",
        "\n",
        "    # Filter stances to select the most relevant one\n",
        "    stance = response.choices[0].message.content\n",
        "    if stance[-1]==\".\" :\n",
        "      stance = stance.rstrip(stance[-1])\n",
        "    #if stance in [\"unrelated\", \"discuss\", \"agree\", \"disagree\"]:\n",
        "    #    return stance\n",
        "    #else:\n",
        "    #    return None\n",
        "    return stance"
      ],
      "metadata": {
        "id": "BBamBYZ6KeAb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Few-shot prompt\n",
        "print(dataset_test.stances[248])\n",
        "print(\"The stance given by GPT3.5 is: \"+classify_stance_few_shot(dataset_test.articles[dataset_test.stances[248]['Body ID']], dataset_test.stances[248]['Headline']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada50c5d-ae91-48cb-c179-bfd9a847eb4d",
        "id": "hFaIDFj9KlY2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Headline': '‘Crabzilla’ spotted off the coast of Britain', 'Body ID': 893, 'Stance': 'disagree'}\n",
            "The stance given by GPT3.5 is: disagree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classify the test dataset, we write the GPT responses to a csv file.\n",
        "import time\n",
        "filename = \"/content/FakeNewsChallenge/result/\"+\"few_shot_prompt_prediction\"+str(date.today())+\".csv\"\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "    fieldnames = [\"STANCE_INDEX\",\"ACTUAL_STANCE\",\"PREDICT_STANCE\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in testset_index:\n",
        "      actual_stance = dataset_test.stances[i]['Stance']\n",
        "      headline = dataset_test.stances[i]['Headline']\n",
        "      body = dataset_test.articles[dataset_test.stances[i]['Body ID']]\n",
        "      predict_stance = classify_stance_few_shot(dataset_test.articles[dataset_test.stances[i]['Body ID']], dataset_test.stances[i]['Headline'])\n",
        "      writer.writerow({'STANCE_INDEX': str(i), 'ACTUAL_STANCE': str(actual_stance), 'PREDICT_STANCE': str(predict_stance)})\n",
        "      csvfile.flush()\n",
        "      time.sleep(21) #wait for 21s to not interrupt API access"
      ],
      "metadata": {
        "id": "WWwnhBgWrCuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scoring classifier"
      ],
      "metadata": {
        "id": "1_JVR_K52Jgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcaJUOfv2dG0",
        "outputId": "2c845124-52e1-4d94-cd4e-4caf27ff1e36"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, xxhash, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate\n",
            "Successfully installed datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 huggingface-hub-0.14.1 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.3 transformers-4.29.0 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.score import report_score\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "XIJrd34X5N8N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read the generated csv\n",
        "output = pd.read_csv('/content/FakeNewsChallenge/result/zero_shot_prompt_prediction2023-04-26.csv')  "
      ],
      "metadata": {
        "id": "3VIwkUhitVJy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output['PREDICT_STANCE'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZS-YJh2zsTp",
        "outputId": "356cb568-e37e-4555-b81c-57ff80c1b351"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "discuss                                                                                                     602\n",
              "unrelated                                                                                                   173\n",
              "agree                                                                                                       154\n",
              "disagree                                                                                                     64\n",
              "Discuss                                                                                                       6\n",
              "unrelated. (There is no mention of Obama ordering the Fed to adopt the Euro currency in this news body.)      1\n",
              "Name: PREDICT_STANCE, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to map the filled prompt to stance label."
      ],
      "metadata": {
        "id": "9pKhEpMuz3de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.loc[ output['PREDICT_STANCE'] == \"Discuss\", 'PREDICT_STANCE'] = 'discuss'\n",
        "output.loc[ output['PREDICT_STANCE'] == \"unrelated. (There is no mention of Obama ordering the Fed to adopt the Euro currency in this news body.)\", 'PREDICT_STANCE'] = 'unrelated'"
      ],
      "metadata": {
        "id": "UGYd5BLiz1m2"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion matrix for zero-shot learning:\")\n",
        "report_score(output[\"ACTUAL_STANCE\"], output[\"PREDICT_STANCE\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYi4nw-p5g4D",
        "outputId": "ff14f332-d1f2-44c2-dfc6-5b9389f265bd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix for zero-shot learning:\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |    25     |     5     |    44     |     0     |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |     5     |     8     |     4     |     0     |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |    39     |    11     |    125    |     3     |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |    85     |    40     |    435    |    171    |\n",
            "-------------------------------------------------------------\n",
            "Score: 227.75 out of 451.75\t(50.41505257332595%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50.41505257332595"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metric(df):\n",
        "  #unrelated\n",
        "  label2id = {\"unrelated\": 1, \"discuss\": 0, \"agree\": 0, \"disagree\": 0}\n",
        "  df_unrelated = df.replace({\"ACTUAL_STANCE\": label2id})\n",
        "  df_unrelated = df_unrelated.replace({\"PREDICT_STANCE\": label2id})\n",
        "  f1_unrelated = f1.compute(references=df_unrelated[\"ACTUAL_STANCE\"], predictions=df_unrelated[\"PREDICT_STANCE\"])\n",
        "  #discuss\n",
        "  label2id = {\"unrelated\": 0, \"discuss\": 1, \"agree\": 0, \"disagree\": 0}\n",
        "  df_discuss = df.replace({\"ACTUAL_STANCE\": label2id})\n",
        "  df_discuss = df_discuss.replace({\"PREDICT_STANCE\": label2id})\n",
        "  f1_discuss = f1.compute(references=df_discuss[\"ACTUAL_STANCE\"], predictions=df_discuss[\"PREDICT_STANCE\"])\n",
        "  #agree\n",
        "  label2id = {\"unrelated\": 0, \"discuss\": 0, \"agree\": 1, \"disagree\": 0}\n",
        "  df_agree = df.replace({\"ACTUAL_STANCE\": label2id})\n",
        "  df_agree = df_agree.replace({\"PREDICT_STANCE\": label2id})\n",
        "  f1_agree = f1.compute(references=df_agree[\"ACTUAL_STANCE\"], predictions=df_agree[\"PREDICT_STANCE\"])\n",
        "  #disagree\n",
        "  label2id = {\"unrelated\": 0, \"discuss\": 0, \"agree\": 0, \"disagree\": 1}\n",
        "  df_disagree = df.replace({\"ACTUAL_STANCE\": label2id})\n",
        "  df_disagree = df_disagree.replace({\"PREDICT_STANCE\": label2id})\n",
        "  f1_disagree = f1.compute(references=df_disagree[\"ACTUAL_STANCE\"], predictions=df_disagree[\"PREDICT_STANCE\"])\n",
        "\n",
        "  #f1 macro\n",
        "  f1_macro = (f1_unrelated['f1']+f1_discuss['f1']+f1_agree['f1']+f1_disagree['f1'])/4\n",
        "  \n",
        "  return {'f1_macro': f1_macro, 'f1_unrelated': f1_unrelated['f1'],'f1_discuss': f1_discuss['f1'],'f1_agree': f1_agree['f1'],'f1_disagree': f1_disagree['f1']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "30413cc7d415478cadc636ba589d2bbf",
            "8598aa0b3fce41a78fb4c719aa6b2a75",
            "c50323aa7fc94278a2f42dd347b3a5dd",
            "2eac8612768042a188f0a408ee02b06c",
            "79f7bd98a38e41e5acf12748b76c62de",
            "813c76f75a004719acbf75d41588f867",
            "fb13529f200148c89ab042699ae429bd",
            "b45bb91236f644bab0e29ea0601c9c60",
            "12162002c533420ca84cd6e0dfacbe70",
            "bce219a5e2564a1bbfcf8f0a4f10bca9",
            "505704d2798a4801a0bb3223b220d806"
          ]
        },
        "id": "EQqxkM7Z2Ng3",
        "outputId": "8b90b4be-b5c9-4641-e1b7-3bf4ee8aed32"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30413cc7d415478cadc636ba589d2bbf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Other metrics for zero-shot learning:')\n",
        "compute_metric(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO32Himt2zA5",
        "outputId": "d9e8942c-c9e8-4c96-b2c8-d4e258db907f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other metrics for zero-shot learning:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1_macro': 0.27819895501464204,\n",
              " 'f1_unrelated': 0.37790055248618787,\n",
              " 'f1_discuss': 0.31806615776081426,\n",
              " 'f1_agree': 0.21929824561403508,\n",
              " 'f1_disagree': 0.19753086419753085}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}